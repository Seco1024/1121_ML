{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.array([(-3, -2.5), (-1, 0.8), (1, 4), (3, 2), (5, 5.3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_error(D, m, b):\n",
    "    total_error = 0\n",
    "    for x, y in D:\n",
    "        total_error += (y - (m * x + b)) ** 2\n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "問題一解答是: 9.112499999999997\n"
     ]
    }
   ],
   "source": [
    "print(f\"問題一解答是: {total_error(D, 0.97, 1.1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取total error 公式如下:\n",
    "def total_error(D, m, b): \n",
    "    total_error = 0 # 一開始的 total error 為 0\n",
    "    for x, y in D: # 對於每一個點\n",
    "        total_error += (y - (m * x + b)) ** 2 # 計算 error 並加到 total error 上，每個點的 error 代表預測值與實際值的差距的平方\n",
    "    return total_error #回傳 total error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題三"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 梯度下降的算式如下:\n",
    "## total error = (y - (m * x + b)) ** 2\n",
    "## d(total error)/dm = -2 * x * (y - (m * x + b)) \n",
    "## d(total error)/db = -2 * (y - (m * x + b)) \n",
    "## 用上方式子尋找可以使得 total error 最小的 m, b，接下來都用程式計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(m, b, D, lr, n_iter):\n",
    "    for i in range(n_iter):\n",
    "        w0_grad = 0\n",
    "        w1_grad = 0\n",
    "        \n",
    "        for x, y in D:\n",
    "            w0_grad += -2 * (y - (m * x + b))\n",
    "            w1_grad += -2 * x * (y - (m * x + b))\n",
    "        m = m - lr * w1_grad\n",
    "        b = b - lr * w0_grad\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8400000000000002 1.0799999999999987\n"
     ]
    }
   ],
   "source": [
    "w1, w0 = gradient_descent(0, 0, D, 0.01, 100000)\n",
    "print(w1, w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: w1*=1.357  w2*=0.283"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題四"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "問題四中，我們的模型的loss是: 8.324000000000002\n"
     ]
    }
   ],
   "source": [
    "print(f\"問題四中，我們的模型的loss是: {total_error(D, 0.84, 1.08)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示專家模型的 total error 比較大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題五"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較程序:\n",
    "#1. 將新資料納入資料集中\n",
    "#2. 重新計算我們的 w0, w1\n",
    "#3. 比較兩個模型的 total error，較小者為較好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我們的模型的loss是: 8.730702132831015\n",
      "專家模型的loss是: 10.953445999999998\n"
     ]
    }
   ],
   "source": [
    "D = np.array([(-3, -2.5), (-1, 0.8), (1, 4), (3, 2), (5, 5.3), (2.5, 2.5), (3.7, 3.8)])\n",
    "w1, w0 = gradient_descent(0, 0, D, 0.01, 100000)\n",
    "print(f\"我們的模型的loss是: {total_error(D, w1, w0)}\")\n",
    "print(f\"專家模型的loss是: {total_error(D, 0.97, 1.1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得結果:我們的模型總誤差較小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
